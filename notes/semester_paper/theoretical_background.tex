\chapter{Theoretical Background}

This chapter provides an overview and an intuition on the field of missing
data. It mainly follows \cite{schafer2002missing},
\cite{little2002statistical}, \cite{van2012flexible}, with some input from
\cite{wikipediaImputation2015}, \cite{matloffblog2015}, \cite{gelman2006data},
\cite{troyanskaya2001missing}. This chapter begins with a short description on
the nature missingness, then describes several procedures in order to handle
missing data.

\section{Mechanism of missingness}
\label{sec:source-missingness}

\cite{van2012flexible} describes two concepts helping us to understand how to
solve the problem of missing data: intentional and unintentional missingness,
as well as unit and item missingness. The experimenter can decide to not
measures all possible variables in an experiment and encode his decisions as
missing observations. This is a reasonable decision if the cost of measuring
variables is material and unnecessary for some experimental case, such as in
medical experimentation. However, it might also happen that the experimenter
could not measure some variable, e.g. when a respondent to a survey refuse to
answer to some questions. In this case, the missingness is named
unintentional. The second concept of missingness is about unit and items: one
says a unit is missing when none of the variables of interest could be
measured, whereas item missinginess refers to some variable missing.

In order to complete missing data, assumptions need to be taken about the
underlying mechanism creating missing observations: missing completely at
random (MCAR), missing at random (MAR) and missing not at random (MNAR).

\paragraph{Notation}

Let $Y \in \mathbb{R}^{n\times p}$ be the data matrix containing missing data
for $n$ observations with $p$ variables,
$R = (R_{ij})_{i,j=1}^{n,p} \in \{0, 1\}^{n\times p}$ denotes the response
$y_{ij}$ (i.e. $R_{ij} = 1$ is $y_{ij}$ is observed, and is $0$
otherwise). $Y_{obs}$ and $Y_{mis}$ denote observations which are observed,
respectively, missing, such that $Y=(Y_{obs}, Y_{mis})$. Note that we always
observe $R$ and $Y_{obs}$ whereas we usually do not have $Y_{mis}$.

\paragraph{MCAR}

The data are said to be \emph{MCAR} if, for all $i \in \{1, \dots, n\}$ and
$j \in \{1, \dots, p\}$,
\begin{align*}
P(R_{ij}=0 \; \vert \;  Y_{obs}, Y_{mis}) = P(R_{ij}=0),
\end{align*}
or equivalently
\begin{align*}
  P(Y = y \; \vert \;  R_{ij}=r) = P(Y=y), \; y \in \mathbb{R}^{n\times p}, \; r \in \{0, 1\}.
\end{align*}
It means the probability of being missing depends does not depends on the
actual value of $Y$.

\paragraph{MAR} For multiple imputation, one requires only $R_{ij} \perp Y_{mis}$,
for all $i \in \{1, \dots, n\}$ and $j \in \{1, \dots, p\}$, that is
\begin{align*}
  P(R_{ij}=0 \; \vert \;  Y_{obs}, Y_{mis}) = P(R_{ij}=0 \; \vert \;  Y_{obs}),
\end{align*}
that is other observed variables impact of the probability of missingness but
the missing mechanism only depends on the observed variables and not the actual
missing value. In this case, we say the data $Y$ are \emph{MAR}.

\paragraph{MNAR}
The data are MNAR if for any valid pair of indices $(i, j)$,
\begin{align*}
  P(R_{ij}=0 \; \vert \;  Y_{obs}, Y_{mis})
\end{align*}
can not be simplified. It essentially means that the rate of response depends
on the actual value of the missing observations. The standard example is the
survey about salary when people with high salary tend to hide their
earnings.

Modern statistical technique can handle MNAR and MAR cases, whereas simple
technique only MCAR, which is quite restrictive.

\section{Statistical completion}
\label{sec:stand-appr-miss}

\paragraph{Complete case analysis}
Unfortunately, One of the most used technique to cope with missing data: the
researcher only keeps observation that are complete. This might lead to valid
analysis, as the method does not introduce any bias if the missing values are
uniformly distributed. Nevertheless, this methodology can not work in modern
settings where the probability of one missing variable is quite high: Many data
points would be discarded.

\paragraph{Pairwise deletion}

This methods improve from the previous one by deleting observations only if the
variable which is missing must be used in the model. This is typically relevant
for computing correlation for example, although some care must be taken in this
case, as the resulting correlation matrix might not be semi-positive definite
anymore.

\paragraph{Single imputation}

The data matrix is sorted according to some order, \emph{last observation
  carried forward} is the method of replacing the missing value with last valid
value. The missing value can also be replaced with the mean of the other
observations, however, correlations are attenuated. Regression imputation use
the other variables as predictors to replace the missing value, although
precision is misleadingly augmented, hence does not reflect the statistical
errors of the missing data. This problem is partially solved by multiple
imputation.

\paragraph{Multiple imputation}

Under the MAR assumption, the multiple imputation (MI) is similar to
bootstrapping method: the distribution of each variable conditional and the
others is fitted, then in case of missing value, a sample is drawn from this
distribution. The desired statistics are averaged except for the standard error
which is constructed by adding the variance of the imputed data and the within
variance of each data set. The last step solves the problem of understating
uncertainty. Standard errors reflect missing-data uncertainty and finite-sample
variation.

More precisely, in the one-dimensional case, if the sample is large enough so
that the estimator $Q$ follows a Gaussian distribution, then the estimate
$\hat Q$ and the standard error $T$ can be computed from the estimates of
$(Q^j, U^j)_{j=1}^m$, $Q^j$, respectively, $U^j$ being the fitted value of $Q$,
respectively the standard error, for data sets $j$:
\begin{align*}
  \hat Q & = m^{-1} \sum_{j=1}^m Q^j, \\
  \hat U & = m^{-1} \sum_{j=1}^m U^j, \\
  B & = (m-1)^{-1}\sum_{j=1}^m(Q^j - \hat Q)^2, \\
  T & = \hat U + (1 + m^{-1}) B.
\end{align*}

For confidence interval, the Student's $t$ approximation can be used with the
degree of freedom given by
\begin{align*}
\nu = (m-1)\Big[1 + \frac{\hat U}{(1 + m^{-1})B} \Big]^2.
\end{align*}

The estimated rate of missing information for $Q$ is approximately
$\tau/(\tau+1)$ where $\tau = (1 + m^{-1})B/\hat U$, the relative increase in
variance due to non-response. See \cite{schafer1997analysis} for more cases.

An advantage of MI is the number of need imputation: the efficiency based on
$m$ samples relative o an infinite number is $(1 + \lambda/m)^{-1}$, where
$\lambda$ is the rate of missing information, which measures the increase in
the large-sample variance of a parameter estimate due to missing values. $m=20$
is often good in practice.

Obviously, the missing values problem is dealt before the analysis with MI, in
contrast with maximum likelihood estimation. The danger from MI is the ability
to use different models for imputation and analysis, which might lead to
inconsistency.

\section{Algorithmic completion}
\label{sec:compl-case}

The advantage of multiple imputation is the framework provides tools to account
for the uncertainty about the estimated quantities, uncertainty introduced by
the completion mechanism. In contrast, algorithmic methods do not offer such
information, but they are often simpler, faster and more flexible. Singular
value decomposition and nearest-neighbors are two common techniques.

\paragraph{Singular value decomposition} Singular values of a matrix $Y$ are
the square root of the non-negative eigenvalues of $Y^TY$ and the singular
value decomposition (SVD) is provided by
\begin{align}\label{eq:svd}
\hat Y^c_J = U_JD_JV_J^T,
\end{align}
where $D_J \in \mathbb{R}^{N \times p}$ is a diagonal matrix containing the
leading $J < p$ singular values of $Y^c$ and $V_J \in \mathbb{R}^{p \times p}$
and $U_J \in \mathbb{R}^{N \times N}$ is the corresponding orthogonal matrix of
$J$ right and left singular vectors. It can be proved that $\hat Y^c$ is the
nearest matrix of $Y^c$ among matrices with rank $J$ with respect to the sum of
squares norm $\vert \vert A \vert \vert ^2 = tr(AA^T)$.

If $y_i$ is any row of $Y^c$, consider the regression of the $p$ values in
$y_i=(y_{i1}, \dots, y_{ip})^T$ on the eigen-vectors $v_1, \dots, v_J$, each $p$
dimensional vectors. The regression solves
\begin{align*}% \label{eq:compleeccaseSvd}
\min_{\beta} \vert\vert y_{i} - V_j\beta \vert\vert^2 =
  \min_{\beta} \sum_{l=1}^p \big(y_{il} - \sum_{j=1}^J v_{lj}\beta_j \big)^2,
\end{align*}
with solution $\hat \beta = (V_J^T V_J)^{-1} V_J^T Y = V_J^T Y$ (since $V_J$ is
orthogonal) and orthogonal values $\hat y_l = V_l\hat\beta, l \in \{1, \dots, J
\}$. Thus, according to Equation \eqref{eq:svd}, $Y^cV_J = U_JD_j$ gives all
the (transposed) regression coefficients for all the rows and $\hat Y^c =
U_JD_JV_J^T$ all the fitted values. Hence, once the matrix $V_J$ is computed,
SVD approximate each row of $Y^c$ by its fitted vector obtained by regression
(or projection) on $V_J$. This suggest for a row $y_i$ of $Y_{mis}$ with some missing
components, they could possibly be imputed from
\begin{align*}
\min_{\beta} \sum_{l=1}^p 1(R_{il}=1) \big(y_{il} - \sum_{j=1}^J v_{lj}\beta_j \big)^2,
\end{align*}
where $R_{il}$ is the response indicator of $y_{il}$.

The imputation procedure can thus be described as the following.
\begin{enumerate}
\item Compute the SVD of $Y^c$ and keep $V_J$.
\item For a row $y^*$ with any missing element, compute
  \begin{align*}
    \hat\beta^* = ({V_J^{*}}^{T} V_J^*)^{-1} {V_J^*}^{T} y^*,
  \end{align*}
  where $V_j^*$ is the shortened version of $V_J$ with the appropriate rows
  removed (corresponding the missing elements of $y^*$). Note $V_J^{*}$ no
  longer has orthogonal columns.

\item The predictions of the missing elements are $V_J^{(*)}\hat\beta^*$ where
  $V_J^{(*)}$ is the complement in $V_J$ of $V_J^{*}$.
\end{enumerate}

Usually, the data matrix is centered before SVD, however, for missing data, an
intercept has to be fitted and a method based simulation is provided
afterwards. The previous methods usually discards a great number of data,
particularly when $p >> N$. In contrast, the next iterative procedure
circumvent the problem at the cost of more computation.

\begin{enumerate}[(1)]
\item  Set $y^*$ as $Y$ with all missing values filled by the mean of their
  row.
\item \label{enumerate:step:svd:begin} Solve the problem
  \begin{align} \label{eq:completesvd}
    \min_{V_J, D_J, U_J} \vert \vert Y^* - m 1^T - U_J D_J V_J^T \vert \vert^2_F
  \end{align}
  where $\vert\vert \cdot \vert\vert^2_F$ is the sum of squares of all non-missing
  elements and $m \in \mathbb{R}^N$ is the row means of $Y^*$.
\item Predict the missing values of $Y$ with the fitted values.
\item Reset $Y^*$ as $Y$ with the missing values replaced by the result of
  previous step.
\item \label{enumerate:step:svd:end} Repeat steps
  \ref{enumerate:step:svd:begin}-\ref{enumerate:step:svd:end}, until the size
  of the relative update of the missing values become negligible.
\end{enumerate}

According to \cite{hastie1999imputing}, only 6 iterations are
necessary. Interestingly, the solution of Equation \eqref{eq:completesvd} is a
fixed point, i.e. if missing values are filled, and the SVD algorithm is
executed on the complete matrix, the solution remains identical.


\paragraph{Soft-impute completion}

The representation of the data $Y$ described in Equation \eqref{eq:svd}
can be softened a little bit to get faster and more stable completion. In order
to do so, define $\mathcal{P}_\Omega(Y)$ as the operator that \emph{projects}
entries in $Y$ with indices not in $\Omega$ to $0$ and keep the other elements
unchanged. If $\Omega$ is the set of indices $(i, j)$ where $Y$ have
non-missing values (i.e. $R_{ij}=1$), then $\Omega^{\perp}$ is the set of
indices where $Y$ has missing value and $\mathcal{P}_{\Omega}(Y)$ is a version of
$Y$ where missing value have been replaced with $0$. The data completion
problem can be interpreted as finding a matrix $M$ minimizing
\begin{align*}
\frac{1}{2} \vert\vert \mathcal{P}_\Omega(Y) - \mathcal{P}(M) \vert\vert_F^2 + \lambda \vert \vert M \vert \vert_*,
\end{align*}
where $\vert\vert M \vert \vert^2_F$ is the sum of the squared elements of $M$
and $\vert \vert M \vert \vert_*$ is the sum of singular value of $M$ (also
called the nuclear norm). If $Y^*$ solves this problem then it satisfies the
following condition
\begin{align*}
  Y^* = S_{\lambda}(Z),
\end{align*}
where
\begin{align*}
Z = \mathcal{P}_{\Omega}(Y) + \mathcal{P}_{\Omega^{\perp}}(Y^*).
\end{align*}
and the operator $S_\lambda(Z)$ is defined as following.

\begin{enumerate}
\item Find the SVD decomposition of $Z = UDV^T$ and let $d_i$ be the singular
  value of $D$.
\item Put a soft threshold on the singular values, that is, define
  \begin{align*}
    d_i^* = (d_i - \lambda)_+
  \end{align*}
\item Reconstruct $S_\lambda(Z) = UD^*V^T$. This is called the
  \emph{soft-thresholded SVD}. A sufficiently large $\lambda$ reduce the ranks
  of $D^*$ and consequently of $S_\lambda(Z)$ as well.
\end{enumerate}
$Z$ is thus a completed version of $Y$, with missing value filled in. For small
matrices, this is computationally feasible and for large matrices, consult
\ref{hastie2015softimpute} for the methodology using sparse representation of matrices.

\paragraph{K-nearest neighbors completion}

\cite{troyanskaya2001missing} presents the other end of the spectrum in term of
data usage: \emph{K-nearest neighbor averaging}. The algorithm is described as
following.
\begin{enumerate}
  \item Computed the Euclidean distance between $y^*$ and all the rows in $Y^c$,
    using only those co-ordinates not missing in $y^*$. Identify the $K$ closest
    observations.
  \item Impute the missing coordinates of $y^*$ by averaging the corresponding
    coordinates of the $K$ closest with weights proportional to their distances
    to $y^*$.
\end{enumerate}

Empirically, the number of neighbors $K$ between $5$ to $10$ is often a good
choice for most data set.

%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "semester_paper_sfs.tex"  ***
%%% End: ***
%%% reftex-default-bibliography: ("biblio.bib")
