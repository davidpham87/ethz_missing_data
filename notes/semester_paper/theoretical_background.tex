\chapter{Theoretical Background}

This chapter provides an overview and an intuition on the field of missing
data. It follows mainly \cite{schafer2002missing},
\cite{little2002statistical}, \cite{van2012flexible}, with some impute from
\cite{wikipediaImputation2015}, \cite{matloffblog2015}, \cite{gelman2006data},
\cite{troyanskaya2001missing}.

This chapter begins with a short description on the nature missingness, then
describes several procedures in order to handle missing data.

\section{Mechanism of missingness}
\label{sec:source-missingness}

\cite{van2012flexible} describes two concepts helping us to understand how to
solve the problem of missing data: intentional and unintentional missingness,
as well as unit and item missingness. The experimenter can decide to not
measures all possible variable in an experiment and encode his decisions as
missing observations. This is a reasonable decision if the cost of measuring
variables is material and unnecessary for some experimental case, such as in
medical experimentation. However, it might also happen that the experimenter
could not measure some variable, e.g. when a respondent to a survey refuse to
answer to some question. In this case, the missingness is named
unintentional. The second concept is often missingness is about unit and items:
one says a unit is missing when none of the variables of interest could be
measured, whereas item refers to some variable missing.

In order to complete missing data, assumptions need to be taken about the
underlying mechanism creating missing observations: missing completely at
random (MCAR), missing at random (MAR) and missing not at random (MNAR).

\paragraph{Notation}

Let $Y \in \mathbb{R}^{n\times p}$ be the data matrix containing missing data
for $n$ observations with $p$ variables,
$R = (r_{ij})_{i=1,j=1}^{n,p} \in \{0, 1\}^{n\times p}$ denotes the response
$y_{ij}$ (i.e. $r_{ij} = 1$ is $y_{ij}$ is observed, and is $0$
otherwise). $Y_{obs}$ and $Y_{mis}$ denote the observation that is observed,
respectively, missing, such that $Y=(Y_{obs}, Y_{mis})$. Note that we always
observed $R$ whereas we usually do not have $Y_{mis}$.

\paragraph{MCAR}

The data are said to be \emph{MCAR} if
\begin{align*}
P(R=0 \vert Y_{obs}, Y_{mis}) = P(R=0),
\end{align*}
or equivalently
\begin{align*}
P(Y = y \vert R=i) = P(Y=y),\; i \in \{0, 1\},\; y \in \mathbb{R}^{n\times p}.
\end{align*}
It means the probability of being missing depends does not depends on the
actual value of $Y$.

\paragraph{MAR} For multiple imputation, one requires only $R \perp Y_{mis}$,
that is
\begin{align*}
  P(R=0 \vert Y_{obs}, Y_{mis}) = P(R=0 \vert Y_{obs}),
\end{align*}
that is other observed variables impact of the probability of missingness but
the missing mechanism only depends on the observed variables and not the actual
missing value. In this case, we say the data $Y$ are \emph{MAR}.

\paragraph{MNAR}
The data are MNAR if
\begin{align*}
  P(R=0 \vert Y_{obs}, Y_{mis})
\end{align*}
can not be simplified. It essentially means that the rate of response depends
on the actual value of the missing observations. The standard example is the
survey about salary where people with extremely high salary tends to hide their
earnings.

Modern statistical technique can handle MNAR and MAR cases, whereas simple technique only
MCAR, which is quite restrictive.

\section{Statistical completion}
\label{sec:stand-appr-miss}

\paragraph{Complete case analysis}
Unfortunately, One of the most used technique to cope with missing data: the
researcher only keeps observation that are complete. This might lead to valid
analysis, as the method does not introduce any bias if the missing values are
uniformly distributed. Nevertheless, this methodology can not work in modern
settings where the probability of one missing variable is quite high: Many data
points would be discarded.

\paragraph{Pairwise deletion}

This methods improve from the previous one by deleting observations only if the
variable which is missing must be used in the model. This is typically relevant
for computing correlation for example, although some care must be taken in this
case.

\paragraph{Single imputation}

The data matrix is sorted according to some order, \emph{last observation
  carried forward} is the method of replacing the missing value with last valid
value. The missing value can also be replaced with the mean of the other
observations, however, correlations are attenuated. Regression imputation use
the other variables as predictors to replace the missing value, although
precision is misleadingly augmented, hence does not reflect the statistical
errors of the missing data. This problem is partially solved by multiple
imputation.

\paragraph{Multiple imputation}

Under the MAR assumption, the multiple imputation (MI) is similar to
bootstrapping method: the distribution of each variable conditional and the
others is fitted, then in case of missing value, a sample is drawn from this
distribution. The desired statistics are averaged except for the standard error
which is constructed by adding the variance of the imputed data and the within
variance of each data set. The last step solves the problem of understating
uncertainty. Standard errors reflect missing-data uncertainty and finite-sample
variation.

More precisely, in the one-dimensional case, if the sample is large enough so
that the estimator $Q$ follows a Gaussian distribution, then the estimate
$\hat Q$ and the standard error $T$ can be computed from the estimates of
$(Q^j, U^j)_{j=1}^m$, $Q^j$, respectively, $U^j$ being the fitted value of $Q$,
respectively the standard error, for data sets $j$:
\begin{align*}
  \hat Q & = m^{-1} \sum_{j=1}^m Q^j, \\
  \hat U & = m^{-1} \sum_{j=1}^m U^j, \\
  B & = (m-1)^{-1}\sum_{j=1}^m(Q^j - \hat Q)^2, \\
  T & = \hat U + (1 + m^{-1}) B.
\end{align*}

For confidence interval, the Student's $t$ approximation can be used with the
degree of freedom given by
\begin{align*}
\nu = (m-1)\Big[1 + \frac{\hat U}{(1 + m^{-1})B} \Big]^2.
\end{align*}

The estimated rate of missing information for $Q$ is approximately
$\tau/(\tau+1)$ where $\tau = (1 + m^{-1})B/\hat U$, the relative increase in
variance due to non-response. See \cite{schafer1997analysis} for more cases.

An advantage of MI is the number of need imputation: the efficiency based on
$m$ samples relative o an infinite number is $(1 + \lambda/m)^{-1}$, where
$\lambda$ is the rate of missing information, which measures the increase in
the large-sample variance of a parameter estimate due to missing values. $m=20$
is often good in practice.

Obviously, the missing values problem is dealt before the analysis with MI, in
contrast with maximum likelihood estimation. The danger from MI is the ability
to use different models for imputation and analysis, which might lead to
inconsistency.

\section{Algorithmic completion}
\label{sec:compl-case}

\paragraph{Singular value decomposition}

\paragraph{Soft-impute completion}
\paragraph{K-nearest neighbors completion}

Finally, one should not forget why these technique exists:

\begin{quote}
  With or without missing data, the goal of a statistical procedure should be
  to make valid and efficient inferences about a population of interest -- not to
  estimate, predict, or recover missing observations nor to obtain the same
  results that we would have seen with complete data. \cite{schafer2002missing}
\end{quote}


%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "semester_paper_sfs.tex"  ***
%%% End: ***
%%% reftex-default-bibliography: ("biblio.bib")
