\chapter{Theoretical Background}

This chapter provides an overview and an intuition on the field of missing
data. It follows mainly \cite{schafer2002missing},
\cite{little2002statistical}, \cite{van2012flexible}, with some impute from
\cite{wikipediaImputation2015}, \cite{matloffblog2015}, \cite{gelman2006data},
\cite{troyanskaya2001missing}. This chapter begins with a short description on
the nature missingness, then describes several procedures in order to handle
missing data.

\section{Mechanism of missingness}
\label{sec:source-missingness}

\cite{van2012flexible} describes two concepts helping us to understand how to
solve the problem of missing data: intentional and unintentional missingness,
as well as unit and item missingness. The experimenter can decide to not
measures all possible variable in an experiment and encode his decisions as
missing observations. This is a reasonable decision if the cost of measuring
variables is material and unnecessary for some experimental case, such as in
medical experimentation. However, it might also happen that the experimenter
could not measure some variable, e.g. when a respondent to a survey refuse to
answer to some question. In this case, the missingness is named
unintentional. The second concept is often missingness is about unit and items:
one says a unit is missing when none of the variables of interest could be
measured, whereas item refers to some variable missing.

In order to complete missing data, assumptions need to be taken about the
underlying mechanism creating missing observations: missing completely at
random (MCAR), missing at random (MAR) and missing not at random (MNAR).

\paragraph{Notation}

Let $Y \in \mathbb{R}^{n\times p}$ be the data matrix containing missing data
for $n$ observations with $p$ variables,
$R = (R_{ij})_{i=1,j=1}^{n,p} \in \{0, 1\}^{n\times p}$ denotes the response
$y_{ij}$ (i.e. $r_{ij} = 1$ is $y_{ij}$ is observed, and is $0$
otherwise). $Y_{obs}$ and $Y_{mis}$ denote the observation that is observed,
respectively, missing, such that $Y=(Y_{obs}, Y_{mis})$. Note that we always
observed $R$ whereas we usually do not have $Y_{mis}$.

\paragraph{MCAR}

The data are said to be \emph{MCAR} if, for all $i \in \{1, \dots, n\}$ and
$j \in \{1, \dots, p\}$,
\begin{align*}
P(R_{ij}=0 \; \vert \;  Y_{obs}, Y_{mis}) = P(R_{ij}=0),
\end{align*}
or equivalently
\begin{align*}
  P(Y = y \; \vert \;  R_{ij}=r) = P(Y=y), \; y \in \mathbb{R}^{n\times p}, \; k \in \{0, 1\}.
\end{align*}
It means the probability of being missing depends does not depends on the
actual value of $Y$.

\paragraph{MAR} For multiple imputation, one requires only $R_{ij} \perp Y_{mis}$,
for all $i \in \{1, \dots, n\}$ and $j \in \{1, \dots, p\}$, that is
\begin{align*}
  P(R_{ij}=0 \; \vert \;  Y_{obs}, Y_{mis}) = P(R_{ij}=0 \; \vert \;  Y_{obs}),
\end{align*}
that is other observed variables impact of the probability of missingness but
the missing mechanism only depends on the observed variables and not the actual
missing value. In this case, we say the data $Y$ are \emph{MAR}.

\paragraph{MNAR}
The data are MNAR if for any valid pair of indices $(i, j)$,
\begin{align*}
  P(R_{ij}=0 \; \vert \;  Y_{obs}, Y_{mis})
\end{align*}
can not be simplified. It essentially means that the rate of response depends
on the actual value of the missing observations. The standard example is the
survey about salary where people with extremely high salary tends to hide their
earnings.

Modern statistical technique can handle MNAR and MAR cases, whereas simple technique only
MCAR, which is quite restrictive.

\section{Statistical completion}
\label{sec:stand-appr-miss}

\paragraph{Complete case analysis}
Unfortunately, One of the most used technique to cope with missing data: the
researcher only keeps observation that are complete. This might lead to valid
analysis, as the method does not introduce any bias if the missing values are
uniformly distributed. Nevertheless, this methodology can not work in modern
settings where the probability of one missing variable is quite high: Many data
points would be discarded.

\paragraph{Pairwise deletion}

This methods improve from the previous one by deleting observations only if the
variable which is missing must be used in the model. This is typically relevant
for computing correlation for example, although some care must be taken in this
case.

\paragraph{Single imputation}

The data matrix is sorted according to some order, \emph{last observation
  carried forward} is the method of replacing the missing value with last valid
value. The missing value can also be replaced with the mean of the other
observations, however, correlations are attenuated. Regression imputation use
the other variables as predictors to replace the missing value, although
precision is misleadingly augmented, hence does not reflect the statistical
errors of the missing data. This problem is partially solved by multiple
imputation.

\paragraph{Multiple imputation}

Under the MAR assumption, the multiple imputation (MI) is similar to
bootstrapping method: the distribution of each variable conditional and the
others is fitted, then in case of missing value, a sample is drawn from this
distribution. The desired statistics are averaged except for the standard error
which is constructed by adding the variance of the imputed data and the within
variance of each data set. The last step solves the problem of understating
uncertainty. Standard errors reflect missing-data uncertainty and finite-sample
variation.

More precisely, in the one-dimensional case, if the sample is large enough so
that the estimator $Q$ follows a Gaussian distribution, then the estimate
$\hat Q$ and the standard error $T$ can be computed from the estimates of
$(Q^j, U^j)_{j=1}^m$, $Q^j$, respectively, $U^j$ being the fitted value of $Q$,
respectively the standard error, for data sets $j$:
\begin{align*}
  \hat Q & = m^{-1} \sum_{j=1}^m Q^j, \\
  \hat U & = m^{-1} \sum_{j=1}^m U^j, \\
  B & = (m-1)^{-1}\sum_{j=1}^m(Q^j - \hat Q)^2, \\
  T & = \hat U + (1 + m^{-1}) B.
\end{align*}

For confidence interval, the Student's $t$ approximation can be used with the
degree of freedom given by
\begin{align*}
\nu = (m-1)\Big[1 + \frac{\hat U}{(1 + m^{-1})B} \Big]^2.
\end{align*}

The estimated rate of missing information for $Q$ is approximately
$\tau/(\tau+1)$ where $\tau = (1 + m^{-1})B/\hat U$, the relative increase in
variance due to non-response. See \cite{schafer1997analysis} for more cases.

An advantage of MI is the number of need imputation: the efficiency based on
$m$ samples relative o an infinite number is $(1 + \lambda/m)^{-1}$, where
$\lambda$ is the rate of missing information, which measures the increase in
the large-sample variance of a parameter estimate due to missing values. $m=20$
is often good in practice.

Obviously, the missing values problem is dealt before the analysis with MI, in
contrast with maximum likelihood estimation. The danger from MI is the ability
to use different models for imputation and analysis, which might lead to
inconsistency.

\section{Algorithmic completion}
\label{sec:compl-case}
For this particular section, the data matrix will be denoted as $X$, in order
to remain consistent with the literature.

\paragraph{Singular value decomposition} Singular values of a matrix $X$ are
the square root of the non-negative eigenvalues of $X^TX$. Singular value
decomposition (SVD) is provided by
\begin{align}\label{eq:svd}
\hat X^c_J = U_JD_JV_J^T,
\end{align}
where $D_J \in \mathbb{R}^{N \times p}$ is a diagonal matrix containing the
leading $J < p$ singular values of $X^c$ and $V_J \in \mathbb{R}^{p \times p}$
and $U_J \in \mathbb{R}^{N \times N}$, the corresponding orthogonal matrix of
$J$ right and left singular vectors. $\hat X^c$ is the nearest matrix of $X^c$
among matrices with rank $J$ with respect to the sum of squares norm $\vert
\vert A \vert \vert ^2 = tr(AA^T)$.

If $x_i$ is any row of $X^c$, consider the regression of the $p$ values in
$x_i=(x_{i1}, \dots, x_{ip})^T$ on the eigen-vectors $v_1, \dots, v_J$, each $p$
dimensional vectors. The regression solves
\begin{align*}% \label{eq:compleeccaseSvd}
\min_{\beta} \vert\vert x_{i} - V_j\beta \vert\vert^2 =
  \min_{\beta} \sum_{l=1}^p \big(x_{il} - \sum_{j=1}^J v_{lj}\beta_j \big)^2,
\end{align*}
with solution $\hat \beta = (V_J^T V_J)^{-1} V_J^T x = V_J^T x$ (since $V_J$ is
orthogonal) and orthogonal values $\hat x_l = V_l\hat\beta, l \in \{1, \dots, J
\}$. Thus, according to Equation \eqref{eq:svd}, $X^cV_J = U_JD_j$ gives all
the (transposed) regression coefficients for all the rows and $\hat X^c =
U_JD_JV_J^T$ all the fitted values. Hence, once the matrix $V_J$ is computed,
SVD approximate each row of $X^c$ by its fitted vector obtained by regression
(or projection) on $V_J$. This suggest for a row $x_i$ of $X^m$ with some missing
components, they could possibly be imputed from
\begin{align*}
\min_{\beta} \sum_{l=1}^p 1(R_{il}=1) \big(x_{il} - \sum_{j=1}^J v_{lj}\beta_j \big)^2,
\end{align*}
where $R_{il}$ is the response indicator of $x_{il}$.

The imputation procedure is described as the following.
\begin{enumerate}
\item Compute the SVD of $X^c$ and keep $V_J$.
\item For a row $x^*$ with missing element, compute
  \begin{align*}
    \hat\beta^* = ({V_J^{*}}^{T} V_J^*)^{-1} {V_J^*}^{T} x^*,
  \end{align*}
  where $V_j^*$ is the shortened version of $V_J$ with the appropriate rows
  removed (corresponding the missing elements of $x^*$). Note $V_J^{*}$ no
  longer has orthogonal columns.

\item The predictions of the missing elements are $V_J^{(*)}\hat\beta^*$ where
  $V_J^{(*)}$ is the complement in $V_J$ of $V_J^{*}$.
\end{enumerate}

Usually, the data matrix is centered before SVD, however, for missing data, an
intercept has to be fitted and a method based simulation is provided
afterwards. The previous methods usually discards a great number of data,
particularly when $p >> N$. In contrast, the next iterative procedure
circumvent the problem at the cost of more computation.

\begin{enumerate}[(1)]
\item  Set $X^*$ as $X$ with all missing values filled by the mean of their
  row.
\item \label{enumerate:step:svd:begin} Solve the problem
  \begin{align} \label{eq:completesvd}
    \min_{V_J, D_J, U_J} \vert \vert X^* - m 1^T - U_J D_J V_J^T \vert \vert^*
  \end{align}
  where $\vert\vert \cdot \vert\vert^*$ is the sum of squares of all non-missing
  elements and $m \in \mathbb{R}^N$ is the row means of $X^*$.
\item Predict the missing values of $X$ with the fitted values.
\item Reset $X^*$ as $X$ with the missing values replaced by the result of
  previous step.
\item \label{enumerate:step:svd:end} Repeat steps
  \ref{enumerate:step:svd:begin}-\ref{enumerate:step:svd:end}, until the size
  of the relative update of the missing values become negligible.
\end{enumerate}

According to \cite{hastie1999imputing}, only 6 iterations are
necessary. Interestingly, the solution of Equation \eqref{eq:completesvd} is a
fixed point, i.e. if missing values are filled, and the SVD algorithm is
executed on the complete matrix, the solution remains identical.

% \paragraph{Soft-impute completion} TODO

% TODO read
\paragraph{K-nearest neighbors completion}

\cite{troyanskaya2001missing} presents the other end of the spectrum in term of
data usage: \emph{K-nearest neighbor averaging}. The algorithm is described as
following.
\begin{enumerate}
  \item Computed the Euclidean distance between $x*$ and all the rows in $X^c$,
    using only those co-ordinates not missing in $x*$. Identify the $K$ closest
    observations.
  \item Impute the missing coordinates of $x^*$ by averaging the corresponding
    coordinates of the $K$ closest with weights proportional to their distances
    to $x*$.
\end{enumerate}

Empirically, the number of neighbors $K$ between $5$ to $10$ is often a good
choice for most data set.

%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "semester_paper_sfs.tex"  ***
%%% End: ***
%%% reftex-default-bibliography: ("biblio.bib")
